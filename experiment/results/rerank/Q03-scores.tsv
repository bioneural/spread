rank	entry_id	cluster	rerank_score	rrf_score	channels	content
1	22	3	0.998624	0.030118	fts+vec	Tuned the screen classifier prompt from a binary yes/no format to a structured c 
2	73	8	0.000000	0.032522	fts+vec	Testing classifier accuracy requires a labeled dataset. The gemma3 1b tuning exp 
3	28	3	0.000000	0.031514	fts+vec	Set all classifier prompts to temperature 0.0 despite non-determinism. Higher te 
4	89	9	0.000000	0.031319	fts+vec	Writing should prefer active voice and concrete nouns. The model classified the  
5	29	3	0.000000	0.030777	fts+vec	System prompts are less effective than user prompts for gemma3 1b. Moving instru 
6	24	3	0.000000	0.030579	fts+vec	A classifier prompt that worked for Python code detection returned false negativ 
7	30	3	0.000000	0.029387	fts+vec	Classifier prompts include a machine-readable output format: respond with exactl 
8	26	3	0.000000	0.02904	fts+vec	The extraction prompt for trick originally said extract all important informatio 
9	25	3	0.000000	0.028624	fts+vec	Prompt templates should avoid negation. Do not classify X as Y is less reliable  
10	27	3	0.000000	0.015385	vec	Temperature 0.0 does not guarantee deterministic output from gemma3 1b. Across 1 
11	78	8	0.000000	0.014706	vec	Test queries are written to cover four categories: high-confidence matches, mixe 
12	23	3	0.000000	0.014493	vec	Zero-shot classification with gemma3 1b fails on edge cases. The model needs at  
13	21	3	0.000000	0.014286	fts	The gemma3 1b model says yes to anything technology-adjacent when given vague co 
14	19	2	0.000000	0.014286	vec	Cosine similarity and L2 distance produce different orderings for the same query 
15	4	1	0.000000	0.014085	fts	screen runs as a standalone classifier behind a Unix pipe interface. Input on st 
16	90	9	0.000000	0.014085	vec	Posts follow a consistent structure: TL;DR, setup, experiment, results, dead end 
17	32	4	0.000000	0.013333	vec	book dispatcher crashed when a task payload contained a single quote. The SQL in 
18	35	4	0.000000	0.013158	vec	The FTS5 tokenizer configuration porter unicode61 handles English morphological  
19	16	2	0.000000	0.012987	vec	nomic-embed-text is trained for search rather than classification. Its vectors c 
20	77	8	0.000000	0.012821	vec	The three-channels experiment tests retrieval quality, not performance. Timing d 
