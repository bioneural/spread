rank	entry_id	cluster	distance	content
1	13	2	0.4198677539825439453	Chose nomic-embed-text over mxbai-embed-large because it runs under 100ms per em 
2	98	10	0.4206366539001464843	Log messages are sentences, not codes. stored entry #42 (decision), 3 triples ex 
3	17	2	0.4214923083782196045	The embedding for a 200-word paragraph and the embedding for a 5-word query live 
4	16	2	0.4230974912643432617	nomic-embed-text is trained for search rather than classification. Its vectors c 
5	89	9	0.4254739582538604736	Writing should prefer active voice and concrete nouns. The model classified the  
6	19	2	0.4371207058429718017	Cosine similarity and L2 distance produce different orderings for the same query 
7	11	2	0.4421198666095733643	nomic-embed-text produces 768-dimensional float vectors. At default ollama setti 
8	23	3	0.4486030936241149903	Zero-shot classification with gemma3 1b fails on edge cases. The model needs at  
9	27	3	0.4496519565582275391	Temperature 0.0 does not guarantee deterministic output from gemma3 1b. Across 1 
10	93	10	0.4522798657417297364	Log entries include the tool name as a structured field, not embedded in the mes 
11	25	3	0.452796250581741333	Prompt templates should avoid negation. Do not classify X as Y is less reliable  
12	14	2	0.4528557062149047851	The embedding API returns a JSON response with an embeddings key containing an a 
13	22	3	0.458979576826095581	Tuned the screen classifier prompt from a binary yes/no format to a structured c 
14	30	3	0.4605397880077362061	Classifier prompts include a machine-readable output format: respond with exactl 
15	28	3	0.4629572033882141113	Set all classifier prompts to temperature 0.0 despite non-determinism. Higher te 
16	12	2	0.4634640812873840333	Batch embedding via the ollama /api/embed endpoint accepts an array of inputs an 
17	73	8	0.4640668630599975586	Testing classifier accuracy requires a labeled dataset. The gemma3 1b tuning exp 
18	78	8	0.4646483063697814941	Test queries are written to cover four categories: high-confidence matches, mixe 
19	29	3	0.4756478071212768554	System prompts are less effective than user prompts for gemma3 1b. Moving instru 
20	35	4	0.4761867821216583252	The FTS5 tokenizer configuration porter unicode61 handles English morphological  
